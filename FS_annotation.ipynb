{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Annotating tweets for location extraction and geocoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This notebook is intended for annotating tweets for computing F-score statistics\n",
    "\n",
    "## Dataset structure\n",
    "\n",
    "The tweets dataset are not filtered for a particular topic but cover a range of topics all with different legths. The tweets are all in english language\n",
    "\n",
    "**For F-score test:**  \n",
    "- Determine if a tweet contains a location or not.\n",
    "- If unsure, label the tweet in the class you are more certain of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T14:11:07.448284Z",
     "start_time": "2018-11-19T14:11:07.124074Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Loading Data\n",
    "\n",
    "Insert your annotator id in the annotator_name variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv ('df_location_entities1.csv', nrows=300)\n",
    "df.to_json ('ss_california_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.replace(np.nan, '', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T14:11:07.806330Z",
     "start_time": "2018-11-19T14:11:07.449285Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_file_names = ('ss_california_tweets.json','ss_california_tweets.json')\n",
    "\n",
    "# Remember to replace annotator_name with own names\n",
    "annotator_name = 'Rufai_Vitoria_FS'\n",
    "\n",
    "for fn in dataset_file_names:\n",
    "    print(fn)\n",
    "    df = pd.DataFrame(json.load(open(fn)))\n",
    "    display(df.head()[['text', 'clean_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Annotation\n",
    "\n",
    "### Helper function\n",
    "\n",
    "This function loads the data (using partially annotated .json files if available) and saves it after every annotation.\n",
    "\n",
    "This means that annotation can simply be picked up again whenever desired. Intermediate and final results are saved with the original filename with `_annotated`appended.\n",
    "\n",
    "Only the specified labels (`0,1` by default) are accepted as input, `p` prints a progress bar and any other keys show a help text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T14:18:22.336320Z",
     "start_time": "2018-11-19T14:18:22.041078Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def annotate_tweet_df(fn, possible_labels=('0', '1')):\n",
    "    def process_input(user_input):\n",
    "        if user_input in possible_labels:\n",
    "            return user_input\n",
    "        elif user_input.startswith('p'):\n",
    "            progressbar(compute_annotation_progress(), max_num=len(df))\n",
    "            vc = df[label_column_name].value_counts()\n",
    "            print('labels\\t',  ', '.join([str(k)+': ' + str(v) for k,v in zip(vc.keys(), vc.values)]))\n",
    "        elif user_input.startswith('q'):\n",
    "            raise\n",
    "        else:\n",
    "            print(help_text)\n",
    "\n",
    "        return process_input(input('\\t'))\n",
    "\n",
    "    def compute_annotation_progress():\n",
    "        if label_column_name not in df.keys():\n",
    "            return 0\n",
    "        return len(df) - df[label_column_name].isna().sum()\n",
    "\n",
    "    def progressbar(it, max_num, size=60):\n",
    "        finished = int(round((it / max_num * size))) if it > 0 else 0\n",
    "        rest = size - finished\n",
    "        print('[' + finished * '|' + rest * '.' + ']\\t', it, '/', max_num)\n",
    "\n",
    "    help_text = '\\n'.join(['Possible Commands', str(possible_labels) + '\\tpossible labels',\n",
    "                           'h\\tshow this help', 'p\\tshow progress', 'q\\tquit', ''])\n",
    "\n",
    "    label_column_name = 'label_' + annotator_name\n",
    "    annotated_df_fn = fn.split('.json')[0] + '_annotated' + annotator_name + '.json'\n",
    "\n",
    "    if os.path.exists(annotated_df_fn) and os.path.isfile(annotated_df_fn):\n",
    "        print(annotated_df_fn, 'already exists, continuing previous annotation process')\n",
    "        df: pd.DataFrame = pd.DataFrame(json.load(open(annotated_df_fn)))\n",
    "    else:\n",
    "        df: pd.DataFrame =  pd.DataFrame(json.load(open(fn)))\n",
    "\n",
    "    nb_annotated_tweets = compute_annotation_progress()\n",
    "    if label_column_name in df.keys():\n",
    "        print('Labels from', annotator_name, 'already in data!')\n",
    "        if compute_annotation_progress() < len(df):\n",
    "            print('Continuing annotation,', nb_annotated_tweets, 'of', len(df), 'already annotated')\n",
    "        else:\n",
    "            return\n",
    "    else:\n",
    "        df[label_column_name] = np.nan\n",
    "\n",
    "    print(help_text)\n",
    "    print('Starting annotation for', len(df) - nb_annotated_tweets, 'tweets:')\n",
    "    for index, row in df.iterrows():\n",
    "        if not pd.isna(row[label_column_name]):\n",
    "            continue\n",
    "        print(row.text)\n",
    "        label = process_input(input('\\t'))\n",
    "        if label is not None:\n",
    "            df.loc[index, label_column_name] = label\n",
    "        df.to_json(annotated_df_fn)\n",
    "\n",
    "    print('Finished!\\nSaved results as', annotated_df_fn, '\\n')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Task\n",
    "\n",
    "Please refere to the annotation guide file for annotation examples. In case something is not clear feel free to ask.  \n",
    "\n",
    "IMPORTANT:   \n",
    "- For **F-Score**, we are only interested in the presence or absence of a location within the tweet. The context in which the location is mentioned is not important\n",
    "\n",
    "The label is either `1` if the tweet has a location or `0` otherwise.  \n",
    "\n",
    "For more information see the annotation guide.\n",
    "\n",
    "#### To start annotating run the cell below.  \n",
    "Press q to pause the annotation (the red error is intended bahviour).  \n",
    "Press p to show your progress.  \n",
    "Press h to see all possible functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-19T14:18:27.526842Z",
     "start_time": "2018-11-19T14:18:22.934442Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(map(annotate_tweet_df, dataset_file_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2 = pd.read_json('pd.read_json('ss_california_tweetsRufai_Vitoria_FS.json')')\n",
    "#df2 = pd.read_json('ss_california_tweetsRufai_Vitoria_FS.json', lines=True, orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ss_california_tweets_annotatedRufai_Vitoria_FS.json', 'r') as datafile:\n",
    "    data = json.load(datafile)\n",
    "    df2 = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_values(x):\n",
    "        if len(x) > 2:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['GPE'] = df2['GPE'].apply(parse_values)\n",
    "df2['FAC'] = df2['FAC'].apply(parse_values)\n",
    "df2['ORG'] = df2['ORG'].apply(parse_values)\n",
    "df2['LOC'] = df2['LOC'].apply(parse_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  accuracy_score, average_precision_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "x = df2[['GPE','FAC','ORG','LOC']]\n",
    "Y = df2['label_Rufai_Vitoria_FS']\n",
    "\n",
    "#Splitting X and y into training data and test data within proportion of 80% as training and 20% as test\n",
    "x_train, x_test, Y_train, Y_test = train_test_split(x, Y, test_size = 0.2, random_state=42)\n",
    "\n",
    "#Setting the model and the prediction based on train samples\n",
    "we = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "#Create classifier object\n",
    "we = we.fit(x_train,Y_train)\n",
    "#Train the classifier using the training data\n",
    "\n",
    "#Running the prediction model\n",
    "predictions = we.predict(x_test)\n",
    "predictions\n",
    "\n",
    "#Calculates the score accuracy\n",
    "scoree = accuracy_score(Y_test,predictions)\n",
    "scoree\n",
    "\n",
    "# calculates f1 score\n",
    "f1_score = f1_score(Y_test, predictions , average='macro')\n",
    "\n",
    "# calculates the fbeta-score\n",
    "f_score_recall = fbeta_score(Y_test,predictions, average='macro', beta = 2.5) #Giving more importance to Precision\n",
    "f_score_precision = fbeta_score(Y_test,predictions, average='macro', beta = 0.5) #Giving more importance to Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_score = [[f_score_recall, f_score_precision, f1_score]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df = pd.DataFrame(f_score, columns = ['recall','precision','f1_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('spring_school': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "195px",
    "width": "282px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "metadata": {
   "interpreter": {
    "hash": "b6a006a31bde8fe11737901ba785bcc0588f0bd37819c7193c730d1bfdb0694a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}